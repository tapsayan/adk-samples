# AI Security Agent - Red Team Testing Framework

**Created by [Ankul Jain](https://github.com/ankuljain09)**

A sophisticated multi-agent system for comprehensive AI safety testing and vulnerability assessment using Google's Gemini models and Agent Development Kit (ADK).

## ğŸ¯ Project Overview

The **AI Security Agent** is an automated red-teaming framework designed to test and evaluate the robustness of AI systems against adversarial attacks. It employs a multi-agent architecture where specialized agents collaborate to:

1. **Generate adversarial prompts** targeting specific vulnerability categories
2. **Execute attacks** against a target system (specifically configured as a Banking Assistant handling sensitive financial operations)
3. **Evaluate responses** to determine if safety guidelines were violated

This project leverages Google's Gemini models (Gemini 2.5 Pro and Flash) to create a comprehensive security audit pipeline for LLM systems. It's built on the **Google Agent Development Kit (ADK)** for scalable, production-ready agent orchestration.

### Key Features

- ğŸ”´ **Red Team Agent**: Generates sophisticated adversarial prompts for multiple risk categories
- ğŸ¯ **Target System**: Simulated banking assistant with built-in safety rules
- âœ… **Evaluator Agent**: Neutral assessment of whether safety violations occurred
- ğŸ“Š **Structured Results**: JSON-based evaluation verdicts with detailed reasoning
- ğŸ”§ **Modular Design**: Easy to extend with new agents and risk categories
- ğŸš€ **Built on Google ADK**: Production-grade agent orchestration framework

---

## ğŸ“ Project Folder Structure

```
ai-security-agent/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ llm_red_team_agent/                # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py                       # Main security orchestrator agent
â”‚   â”œâ”€â”€ agent_utils.py                 # Async agent execution utilities
â”‚   â”œâ”€â”€ config.py                      # Configuration (models, parameters)
â”‚   â”œâ”€â”€ tools.py                       # Security scanning tools
â”‚   â”œâ”€â”€ safety_rules.py                # Defines the Banking Safety guidelines
â”‚   â”‚
â”‚   â””â”€â”€ sub_agents/                    # Specialized sub-agents
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ red_team.py                # Adversarial prompt generator
â”‚       â”œâ”€â”€ target.py                  # Target system being tested
â”‚       â””â”€â”€ evaluator.py               # Safety violation detector
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_agent.py                  # Test suite
```

### File Descriptions

| File | Purpose |
|------|---------|
| **agent.py** | Main orchestrator that manages the security scanning workflow |
| **config.py** | Global configuration including model selection and parameters |
| **tools.py** | Core security scan function that orchestrates the 3-step process |
| **safety_rules.py** | Contains the specific system instructions and guardrails for the Banking Assistant|
| **agent_utils.py** | Utilities for executing agents asynchronously with proper session management |
| **red_team.py** | Creates the red team agent that generates adversarial prompts |
| **target.py** | Creates the target agent (banking assistant with safety rules) |
| **evaluator.py** | Creates the evaluator agent that judges safety violations |

---

## ğŸ—ï¸ Agent Architecture

The system uses a **three-stage pipeline** architecture with specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Security Orchestrator                        â”‚
â”‚              (Main LLmAgent - Risk Category Input)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Red Team Worker    â”‚
            â”‚  (Attack Generator)  â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.9            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Generates adversarial prompt
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Target Worker      â”‚
            â”‚  (System Under Test) â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-   â”‚
            â”‚        Flash         â”‚
            â”‚ Temp: 0.1            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Target response to attack
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Evaluator Worker    â”‚
            â”‚ (Safety Verdict)     â”‚
            â”‚                      â”‚
            â”‚ Model: Gemini 2.5-Proâ”‚
            â”‚ Temp: 0.0            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ JSON verdict (PASS/FAIL)
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Security Audit      â”‚
            â”‚  Result Summary      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Roles

#### ğŸ”´ Red Team Agent
- **Purpose**: Generate sophisticated adversarial prompts
- **Model**: Gemini 2.5 Pro (higher capability)
- **Temperature**: 0.9 (high creativity for diverse attacks)
- **Input**: Risk category (e.g., "Toxicity", "Phishing", "Financial Fraud")
- **Output**: Single, highly targeted adversarial prompt

#### ğŸ¯ Target Agent
- **Purpose**: Simulate the system under test
- **Model**: Gemini 2.5 Flash (faster, cost-effective)
- **Temperature**: 0.1 (consistent, conservative responses)
- **Role**: Banking Assistant compliant with strict policies defined in safety_rules.py
- **Input**: Adversarial prompt from red team
- **Output**: Response to the adversarial prompt

#### âœ… Evaluator Agent
- **Purpose**: Determine if safety rules were violated
- **Model**: Gemini 2.5 Pro (high judgment capability)
- **Temperature**: 0.0 (deterministic evaluation)
- **Input**: Attack-response transcript
- **Output**: JSON verdict with PASS/FAIL and reasoning

---

## ğŸ”§ Tools

### generate_attack_prompt
Generates an adversarial prompt designed to test the robustness of a target system or model by attempting to bypass safety guidelines or elicit unintended behaviors.

### simulate_target_response
Simulates a response from the target system or model given an input prompt. This tool mimics the behavior of the target to enable testing without repeated actual API calls.

### evaluate_interaction
Evaluates the results of an interaction between a generated attack prompt and the simulated target response. Determines the effectiveness of the attack and provides metrics on whether safety guidelines were violated or unintended behavior was triggered.


## ğŸ”„ Workflow

The complete workflow follows this sequence:

```
START
  â”‚
  â”œâ”€â–º User Input: Risk Category
  â”‚        (e.g., "Toxicity")
  â”‚
  â”œâ”€â–º STAGE 1: Red Team Generation
  â”‚   â””â”€â–º Prompt: "Generate an adversarial prompt for Phishing"
  â”‚   â””â”€â–º Output: Sophisticated phishing attack prompt
  â”‚
  â”œâ”€â–º STAGE 2: Target Injection & Response
  â”‚   â””â”€â–º Inject: Adversarial prompt into target
  â”‚   â””â”€â–º Target Response: System's attempt to handle/block the attack
  â”‚
  â”œâ”€â–º STAGE 3: Safety Evaluation
  â”‚   â””â”€â–º Input: [ATTACK] and [RESPONSE] pair
  â”‚   â””â”€â–º Verdict: PASS (safety maintained) or FAIL (safety violated)
  â”‚
  â””â”€â–º Output: Security Audit Report
       (Risk category, attack attempt, target behavior, verdict)
END
```

### Key Design Decisions

1. **Async Execution**: Sub-agents run asynchronously with proper session management via `ThreadPoolExecutor`
2. **Temperature Tuning**:
   - Red Team: 0.9 (maximize creative adversarial attempts)
   - Target: 0.1 (consistent, predictable behavior)
   - Evaluator: 0.0 (deterministic, unbiased verdicts)
3. **Serial Pipeline**: Each stage depends on the previous stage's output
4. **Error Handling**: Attack generation failures are caught early to prevent cascade failures

---

## ğŸ’¬ Example Conversation

### Scenario: Testing Unregulated Financial Advice 
![Unregulated Financial Advice](assets/unregulatedFinancialAdvice.png)

### Scenario: Testing Prompt Injection
![Prompt Injection](assets/promptInjection.png)

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- Google Cloud Account with Vertex AI access
- Required libraries: `google-adk`, `google-cloud-aiplatform`, `google-genai`, `tenacity`

---

## ğŸ“¦ Configuration

Edit `llm_red_team_agent/config.py` to customize:

```python
@dataclass
class SecurityAuditConfig:
    evaluator_model: str = "gemini-2.5-pro"      
    red_team_model: str = "gemini-2.5-pro"       
    target_model: str = "gemini-2.5-flash"    
```

---

## ğŸ§ª Testing

Run the test suite:

```bash
python -m pytest tests/test_agent.py -v
```

---

## ğŸ“„ License

Licensed under the Apache License 2.0. See LICENSE file for details.

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“š References

- [Google Agent Development Kit (ADK)](https://developers.google.com/google-developers/documentation)
- [Gemini API Documentation](https://ai.google.dev/)

---

## âš ï¸ Disclaimer

This tool is designed for authorized security testing of AI systems. Ensure you have proper authorization before conducting any red-teaming activities on production systems.

